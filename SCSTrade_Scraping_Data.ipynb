{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_YyCE_hcMeo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tempfile\n",
        "import time\n",
        "import csv\n",
        "from selenium import webdriver\n",
        "from selenium.common.exceptions import TimeoutException, ElementClickInterceptedException, NoSuchElementException\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "\n",
        "# --- Setup Chrome Options ---\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "chrome_options.add_argument(\"--window-size=1920,1080\")\n",
        "temp_user_data_dir = tempfile.mkdtemp()\n",
        "chrome_options.add_argument(f\"--user-data-dir={temp_user_data_dir}\")\n",
        "# --------------------------\n",
        "\n",
        "# --- Initialize WebDriver ---\n",
        "print(f\"Using temporary user data dir: {temp_user_data_dir}\")\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "print(\"WebDriver initialized.\")\n",
        "# ---------------------------\n",
        "\n",
        "url = \"http://www.scstrade.com/MarketStatistics/MS_HistoricalPrices.aspx\"\n",
        "all_data_list = [] # List to store data from all pages\n",
        "header = [] # To store the header row\n",
        "\n",
        "try:\n",
        "    print(f\"Navigating to {url}\")\n",
        "    driver.get(url)\n",
        "    print(\"Page loaded.\")\n",
        "\n",
        "    # --- Placeholders - ACTION: Use the combination you tested ---\n",
        "    company_search_term = \"ENGRO\"\n",
        "    # Make sure this is the exact text from the dropdown you want\n",
        "    company_to_select_text = \"EFERT - Engro Fertilizers Ltd.\" # Use the one that worked\n",
        "    # Use the date range that worked\n",
        "    start_date = \"01/07/2024\" # DD/MM/YYYY\n",
        "    end_date = \"01/10/2025\"   # DD/MM/YYYY\n",
        "    # -------------------------------------------------------------\n",
        "\n",
        "    wait = WebDriverWait(driver, 15)\n",
        "    long_wait = WebDriverWait(driver, 60)\n",
        "\n",
        "    print(\"Locating input fields...\")\n",
        "    company_name_input = wait.until(EC.presence_of_element_located((By.ID, \"tags\")))\n",
        "    start_date_input = driver.find_element(By.ID, \"date1\")\n",
        "    end_date_input = driver.find_element(By.ID, \"date2\")\n",
        "    view_price_button_locator = (By.ID, \"btn1\")\n",
        "    view_price_button_element = driver.find_element(*view_price_button_locator)\n",
        "    print(\"Input fields located.\")\n",
        "\n",
        "    # --- Handle Autocomplete Selection ---\n",
        "    print(f\"Entering Company Search Term: {company_search_term}\")\n",
        "    company_name_input.clear()\n",
        "    company_name_input.send_keys(company_search_term)\n",
        "\n",
        "    autocomplete_dropdown_locator = (By.ID, \"ui-id-1\")\n",
        "    company_option_xpath = f\"//ul[@id='ui-id-1']/li[contains(., '{company_to_select_text}')]\"\n",
        "    company_option_locator = (By.XPATH, company_option_xpath)\n",
        "\n",
        "    try:\n",
        "        print(f\"Waiting for autocomplete option: '{company_to_select_text}'\")\n",
        "        option_element = wait.until(EC.element_to_be_clickable(company_option_locator))\n",
        "        print(\"Autocomplete option found and clickable. Clicking it...\")\n",
        "        option_element.click()\n",
        "        print(f\"Clicked '{company_to_select_text}' from autocomplete.\")\n",
        "        time.sleep(0.5)\n",
        "    except TimeoutException:\n",
        "        print(f\"FATAL: Could not find or click '{company_to_select_text}' in autocomplete.\")\n",
        "        raise\n",
        "    # --- END Autocomplete Handling ---\n",
        "\n",
        "    print(f\"Entering Start Date: {start_date}\")\n",
        "    driver.execute_script(f\"arguments[0].value = '{start_date}';\", start_date_input)\n",
        "\n",
        "    print(f\"Entering End Date: {end_date}\")\n",
        "    driver.execute_script(f\"arguments[0].value = '{end_date}';\", end_date_input)\n",
        "\n",
        "    print(\"Waiting for 'View Price' button to be clickable...\")\n",
        "    wait.until(EC.element_to_be_clickable(view_price_button_locator))\n",
        "\n",
        "    print(\"Attempting standard click on 'View Price' button...\")\n",
        "    try:\n",
        "        view_price_button_element.click()\n",
        "        print(\"Standard click successful.\")\n",
        "        print(\"Pausing briefly for request initiation...\")\n",
        "        time.sleep(2)\n",
        "    except Exception as e:\n",
        "         print(f\"FATAL: Error during standard click: {e}\")\n",
        "         raise\n",
        "\n",
        "    # --- Wait for the *first* page of results to load ---\n",
        "    results_table_locator = (By.ID, \"list\")\n",
        "    data_rows_locator = (By.CSS_SELECTOR, \"#list > tbody > tr.jqgrow\")\n",
        "    no_records_locator = (By.XPATH, \"//div[@id='pager_right']//div[contains(text(), 'No records to view')]\")\n",
        "    loading_indicator_locator = (By.ID, \"load_list\")\n",
        "\n",
        "    # *** FIX: Header Locators and Wait ***\n",
        "    header_row_locator = (By.CSS_SELECTOR, \"#gview_list .ui-jqgrid-htable tr.ui-jqgrid-labels\")\n",
        "    # Locator for the DIVs inside the TH elements which contain the text\n",
        "    header_cell_text_locator = (By.CSS_SELECTOR, \"th div.ui-th-div\")\n",
        "    # Locator for the *first* header cell text div to wait for visibility\n",
        "    first_header_cell_text_locator = (By.CSS_SELECTOR, \"#gview_list .ui-jqgrid-htable th:first-child div.ui-th-div\")\n",
        "    # *** END FIX ***\n",
        "\n",
        "    print(f\"Waiting for first page: loading indicator to disappear... (Timeout: 60s)\")\n",
        "    try:\n",
        "        long_wait.until(EC.invisibility_of_element_located(loading_indicator_locator))\n",
        "        print(\"Loading indicator disappeared for first page.\")\n",
        "\n",
        "        try:\n",
        "             driver.find_element(*no_records_locator)\n",
        "             print(\"Initial search returned 'No records to view'. Exiting.\")\n",
        "        except NoSuchElementException:\n",
        "             print(\"Initial data potentially found. Proceeding to scrape.\")\n",
        "\n",
        "             # *** FIX: Wait for header visibility and extract ***\n",
        "             try:\n",
        "                print(\"Waiting for header row to be visible...\")\n",
        "                # Wait for the first header cell text to be visible specifically\n",
        "                wait.until(EC.visibility_of_element_located(first_header_cell_text_locator))\n",
        "                print(\"Header row visible. Extracting header...\")\n",
        "                # Find the header row element\n",
        "                header_row_element = driver.find_element(*header_row_locator)\n",
        "                # Find the text divs within that row\n",
        "                header_cells = header_row_element.find_elements(*header_cell_text_locator)\n",
        "                header = [cell.text.strip() for cell in header_cells if cell.text.strip()] # Get text from divs\n",
        "                print(\"\\nHeader:\", header)\n",
        "                if not header:\n",
        "                     print(\"ERROR: Header extraction resulted in an empty list. Check selectors/timing.\")\n",
        "                     # Decide if you want to raise an error or try to proceed without header\n",
        "                     # raise ValueError(\"Header extraction failed\")\n",
        "                elif len(header) != 7: # Assuming 7 columns based on data logs\n",
        "                     print(f\"WARN: Expected 7 header columns, but found {len(header)}. Header: {header}\")\n",
        "\n",
        "\n",
        "             except TimeoutException:\n",
        "                print(\"FATAL: Timed out waiting for header to become visible.\")\n",
        "                raise\n",
        "             except Exception as head_ex:\n",
        "                print(f\"FATAL: Error during header extraction: {head_ex}\")\n",
        "                raise\n",
        "             # *** END FIX ***\n",
        "\n",
        "             # --- Pagination Loop ---\n",
        "             page_num = 1\n",
        "             while True:\n",
        "                 print(f\"--- Scraping Page {page_num} ---\")\n",
        "                 time.sleep(0.5)\n",
        "                 current_rows = driver.find_elements(*data_rows_locator)\n",
        "                 print(f\"Found {len(current_rows)} rows on page {page_num}.\")\n",
        "\n",
        "                 if not current_rows and page_num > 1:\n",
        "                     print(\"No more rows found, might be end of pagination.\")\n",
        "                     break\n",
        "\n",
        "                 for row in current_rows:\n",
        "                     cols = row.find_elements(By.TAG_NAME, \"td\")\n",
        "                     data = [col.text.strip() for col in cols]\n",
        "                     # *** FIX: Use the extracted header length for comparison ***\n",
        "                     if header and data and len(data) == len(header):\n",
        "                         all_data_list.append(dict(zip(header, data)))\n",
        "                         # print(\"Data:\", data)\n",
        "                     elif header and data:\n",
        "                         print(f\"WARN: Row data length mismatch on page {page_num}. Header len: {len(header)}, Row len: {len(data)}. Data: {data}\")\n",
        "                     elif not header and data:\n",
        "                         print(f\"WARN: Cannot create dictionary, header extraction failed. Row data: {data}\")\n",
        "                         # Optionally append raw list if header fails but you still want data\n",
        "                         # all_data_list.append(data)\n",
        "\n",
        "\n",
        "                 # --- Check for Next Page ---\n",
        "                 next_button_locator = (By.ID, \"next_pager\")\n",
        "                 try:\n",
        "                     next_button = driver.find_element(*next_button_locator)\n",
        "                     button_classes = next_button.get_attribute(\"class\")\n",
        "                     if \"ui-state-disabled\" in button_classes or \"ui-disabled\" in button_classes:\n",
        "                         print(\"Next button is disabled. Reached the last page.\")\n",
        "                         break\n",
        "                     else:\n",
        "                         print(\"Clicking Next Page button...\")\n",
        "                         # Add slight scroll before click attempt\n",
        "                         try:\n",
        "                             driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button)\n",
        "                             time.sleep(0.2)\n",
        "                         except Exception: pass # Ignore if scroll fails\n",
        "\n",
        "                         next_button.click()\n",
        "                         print(\"Waiting for next page to load...\")\n",
        "                         wait.until(EC.invisibility_of_element_located(loading_indicator_locator))\n",
        "                         print(\"Next page loaded.\")\n",
        "                         page_num += 1\n",
        "                         time.sleep(1)\n",
        "\n",
        "                 except NoSuchElementException:\n",
        "                     print(\"Could not find the Next button. Assuming end of pagination.\")\n",
        "                     break\n",
        "                 except Exception as page_e:\n",
        "                     print(f\"Error during pagination click/wait: {page_e}\")\n",
        "                     # Consider adding a screenshot here too if pagination fails mid-way\n",
        "                     break\n",
        "             # --- End Pagination Loop ---\n",
        "\n",
        "    except TimeoutException:\n",
        "        print(\"FATAL: Timed out waiting for the initial page results to load.\")\n",
        "        raise\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n--- An error occurred ---\")\n",
        "    print(f\"Error type: {type(e).__name__}\")\n",
        "    print(f\"Error message: {e}\")\n",
        "    # ... (screenshot/source saving) ...\n",
        "    try:\n",
        "        screenshot_path = \"error_screenshot.png\"\n",
        "        if 'driver' in locals() and driver:\n",
        "            try:\n",
        "                source_path = \"error_page_source.html\"\n",
        "                with open(source_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                    f.write(driver.page_source)\n",
        "                print(f\"Page source saved to {source_path}\")\n",
        "            except Exception as source_err:\n",
        "                print(f\"Could not save page source: {source_err}\")\n",
        "            driver.save_screenshot(screenshot_path)\n",
        "            print(f\"Screenshot saved to {screenshot_path}\")\n",
        "        else:\n",
        "            print(\"Driver not available to save screenshot/source.\")\n",
        "    except Exception as screen_err:\n",
        "        print(f\"Could not save screenshot/source: {screen_err}\")\n",
        "\n",
        "\n",
        "finally:\n",
        "    # ... (finally block) ...\n",
        "    print(\"\\nClosing browser...\")\n",
        "    if 'driver' in locals() and driver:\n",
        "        driver.quit()\n",
        "        print(\"Browser closed.\")\n",
        "    try:\n",
        "        if 'temp_user_data_dir' in locals() and os.path.exists(temp_user_data_dir):\n",
        "            import shutil\n",
        "            print(f\"Attempting to remove temporary user data dir: {temp_user_data_dir}\")\n",
        "            # shutil.rmtree(temp_user_data_dir)\n",
        "            print(\"Temporary directory removal skipped (commented out).\")\n",
        "    except Exception as cleanup_error:\n",
        "        print(f\"Error cleaning up temp directory: {cleanup_error}\")\n",
        "\n",
        "# --- Post-processing: Save all data to CSV ---\n",
        "# *** FIX: Check header is not empty BEFORE trying to write ***\n",
        "if all_data_list and header: # Ensure header was successfully extracted\n",
        "    output_filename = f\"{company_to_select_text.replace(' ', '_').replace('.', '')}_history_{start_date.replace('/', '-')}_to_{end_date.replace('/', '-')}.csv\"\n",
        "    print(f\"\\nSaving {len(all_data_list)} records to {output_filename}...\")\n",
        "    try:\n",
        "        with open(output_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=header)\n",
        "            writer.writeheader()\n",
        "            writer.writerows(all_data_list)\n",
        "        print(\"Data saved successfully.\")\n",
        "    except Exception as csv_err:\n",
        "        print(f\"Error saving data to CSV: {csv_err}\")\n",
        "elif not header and all_data_list:\n",
        "     print(\"\\nWARNING: Data was scraped but header extraction failed. Cannot save CSV correctly.\")\n",
        "elif not all_data_list:\n",
        "     print(\"\\nNo data was scraped. CSV file not created.\")\n",
        "else:\n",
        "     print(\"\\nUnknown state: Header might be empty or data list empty. CSV not created.\")"
      ]
    }
  ]
}